<!DOCTYPE html>
<html lang="en">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8">
    <!--
        Thanks to Charlie Marsh & Shubhro Saha for making Syndicate.
        Thanks to them and gwern.net for design inspiration
    -->
    <head>
        <title>Inside the Physical Dropbox | David Dohan</title>
        <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans">
        <link href="../static/css/bootstrap.min.css" rel="stylesheet">
        <link href="../static/css/styles.css" rel="stylesheet">
        <!-- Google Analytics -->
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
             m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
             })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-43822184-1', 'ddohan.com');
            ga('send', 'pageview');
        </script>
    </head>
    <body>


    <div id="sidebar">
            <div id="sidebar-links">
                <p>
                <a href="/" title="Home">Home</a>
                <a href="http://www.github.com/dmrd/site" title="Site source">Site</a>
                <a href="/about" title="Who I am">Me</a>
                </p>
                <hr>
                <hr>
                <div id="metadata">
                    <div id="page-created">created:
                        <br>
                        <i>2013-09-03</i></div>
                    <div id="last-modified">modified:
                        <br>
                        <i>2013-09-17</i></div>
                    <br>
                    <i>status: in progress</i></div>
                <hr>
            </div>
        </div>
    </div>

    <div id="content">
        <!--
meta--title--Inside the Physical Dropbox
meta--created--2013-09-03
meta--modified--2013-09-17
meta--status--in progress
-->

<h2 id="looking-inside-the-physical-dropbox">Looking Inside the Physical Dropbox</h2>
<h3 id="the-making-of-a-simple-3d-scanner">The making of a simple 3d scanner</h3>
<div class="figure">
<img src="/scanner/header.jpg" />
</div>
<p>While interning at Dropbox this past summer, I built a “physical dropbox”—a 3d laser scanner inside a Dropbox logo like enclosure—during Dropbox’s biannual hack week with teammates Abhishek Agrawal, Mason Liang, and Rachel Fong. A few weeks later, Makerbot announced the Makerbot Digitizer - a 3d scanner which works on the same principles. In this post, I go through the basic ideas behind how it functions, how we built our own (although not a thorough tutorial), and the results that we achieved.</p>
<p>To start out, here is a picture of the scanner hardware to see how it is laid out:</p>
<div class="figure">
<img src="/scanner/birdseye-dual.jpg" />
</div>
<p>The main features are the turntable that the mug rests on, the camera, and the two lasers on either side.</p>
<h3 id="the-principle">The Principle</h3>
<p>At its most basic level, the scanner shoots a laser line onto an object, captures an image, and then uses the deformation of the laser line from the center of the image to triangulate points on the surface. When this is done along an entire line, we get a slice of the object. When these slices are generated for many small steps through a full rotation of the object, we end up with a point cloud like this one from a scan of a mug:</p>
<p><img src="/scanner/mug.jpg" width="45%"/></p>
<p>So how do we actually triangulate the points? We have a laser, camera, and turntable. We know the distances between each of them, along with the angle at the intersection of the center of the cameras image and the laser line. We then capture an image and clean it up by removing the any part of the laser that overshoots the object and threshold to get the brightest pixels.</p>
<div class="figure">
<img src="/scanner/threshold.jpg" />
</div>
<p>From this image, we pull out a list of (u,v) image coordinates of the middle brightest pixel.</p>
<p>We process these coordinates into a 3d points using two lines of trigonometry—the only real math in the process. For every (u, v) pair at a current turntable rotation of r, we generate an (x, y, z) point as:</p>
<ul>
<li>x = u * cos(r)</li>
<li>y = v</li>
<li>z = -u * sin(r)</li>
</ul>
<h3 id="implementation">Implementation</h3>
<p>Now comes the actual implementation. I am not going to go into the details of every part, but we built our scanner using:</p>
<p>Hardware:</p>
<ul>
<li>Laptop</li>
<li>Arduino mega</li>
<li>Webcam</li>
<li>Two laser modules</li>
<li>Glass stirring rods to spread the laser into a line</li>
<li>Stepper motor and controller</li>
<li>Wood, screws, and felt to line the box</li>
<li>Lots of batteries to power the motors and lasers</li>
</ul>
<p>Software:</p>
<ul>
<li>Python 2.7</li>
<li>OpenCV for capturing and low-level processing of images</li>
<li>Numpy for speeding up image processing</li>
</ul>
<p>The Arduino controls the lasers and stepper motor, and the laptop signals to the Arduino and manages capturing images.</p>
<p>We originally alternated taking pictures and having the motor take a step, but this took much longer than necessary. Instead, we have the motor do a full rotation while the camera takes pictures as fast as it can. The motor signals when it is done, and we process the images with the assumption that the motor turned at a uniform rate. In practice, we found that we usually captured 226 images per rotation in much less time than 30 seconds per laser.</p>
<p>Below is a video showing a scan in progress. We added a second laser to increase scan quality, and we scanned with one laser at a time to avoid dealing with determining which laser line is which.</p>
<iframe src="https://www.facebook.com/video/embed?video_id=10100102584262122"
width="100%" height="400" frameborder="0"></iframe>

<p>The resulting pointclouds are output in the ply format, which can be viewed in most modelling programs. We found that <a href="http://meshlab.sourceforge.net/"><strong>meshlab</strong></a> worked well.</p>
<p>Our code can be found <a href="https://github.com/dmrd/physical-dropbox"><strong>here</strong></a>. Overall, the code is fairly modular, but it became significantly more messy when we decided to add the second laser at 2am on the Friday. The system is entirely commandline based, and a scan is initated by running:</p>
<pre><code>python controller.py scan_and_process scan_name 1 dual</code></pre>
<p>This captures images of 1 rotation with both lasers and processes them. The controller can also only capture, only process, or only one laser if desired.</p>
<h3 id="results">Results</h3>
<p>Here are more example scans that we made while building the scanner:</p>
<p><img src="/scanner/duck.jpg" width="45%"/> <img src="/scanner/honey.png" width="45%"/> <img src="/scanner/jones.png" width="45%"/> <img src="/scanner/sanitizer.png" width="45%"/> <img src="/scanner/bobblehead.png" width="45%"/></p>
<p>It turns out that laser line scanning doesn’t work well with highly reflective or transparent material, so we ended up with a pretty neat effect when we scanned such materials. Three of the non-duck scans above are of a honey bottle, Jones soda bottle, and hand sanitizer. The labels on the bottles are clearly visible, as are the outlines of words on the label. This results from parts of the label absorbing the laser line.</p>
<p>The last scan is a certain bobblehead that Dropboxers may recognize.</p>
<h3 id="comparing-to-the-digitizer">Comparing to the Digitizer</h3>
<h4 id="accuracy">Accuracy</h4>
<p>From looking at scans, it appears that the makerbot scanner has much higher accuracy. Overall, this is expected since our mountings and calibrations were inexact at best. The accuracy would likely be dramatically improved by machining or 3d printing a mount that precisely controlled the distance and angle between the camera and lasers. Additionally, we did not calibrate our camera to account for distortions.</p>
<h4 id="daylight">Daylight</h4>
<p>The Makerbot Digitizer also appears to have hardware to support scanning in daylight. My suspicion is some sort of polarized filter, but it is also possible that increasing the power on our lasers would make it work in light. Please let me know if you are aware of how this works.</p>
<h4 id="nondiffuse-surfaces">Nondiffuse surfaces</h4>
<p>One universal weakness of this technique is that it does not work with reflective objects. Makerbot even has a tutorial on how to scan reflective objects by coating them in a diffuse powder. You can see a result of this effect in some of our scans where the reflective Dropbox name on the mug is visible because the points were not filled in. This is the same effect as on the labels in the example images.</p>
<h4 id="resulting-meshes">Resulting meshes</h4>
<p>You may notice that our scanner ouputs a pointcloud rather than a filled mesh. While we tried various techniques and libraries to triangulate the point cloud into a mesh, we found that most automated approaches produced jagged and irregular meshes. The best strategy was to meshify the output in an external tool such as meshlab. Similarly, we also did not attempt to texture the output since it would require a fully integrate images -&gt; mesh pipeline (although one can store color per vertex rather than as a texture).</p>
<h3 id="improvements-other-techniques">Improvements &amp; Other techniques</h3>
<p>It is possible to improve mesh quality even more by using additional lasers. An alternative that does not require multiple lasers is using a single laser and grating to split it into many beams. Laser scanning as presented here, however, is just one way of scanning objects. There are many of methods for <em>active optical scanning</em>, e.g. systems in which we introduce extra light into the scene. This is in contrast to passive optical techniques such as using the difference between multiple cameras.</p>
<p>Other active techniques include</p>
<ul>
<li>Structured light scanning in which a structured pattern, such as a grid of dots for the Kinect, is projected onto the surface and the deformation observed.</li>
<li>Time of flight scanners use the time for a beam of light to bounce off an object and return to the scanner to calculate depth.</li>
<li>Light diffusion scanners flash lights from many angles and take a picture. For diffuse surfaces, the resulting illumination can be used to reconstruct the surface normals</li>
</ul>
<p>The details of these techniques are interesting, but that’s a topic for another day.</p>
<h3 id="resources">Resources</h3>
<p>If you are curious about other similar 3d scanners, take a look at <a href="https://www.github.com/mvhenten/pylatscan"><strong>pylatscan</strong></a>. It includes blueprints to build a more polished scanner, along with code. I have not used it, but it appears to be fairly well documented.</p>
<p>Another good resource for understanding different techniques is <a href="http://mesh.brown.edu/byo3d/slides.html"><strong>this SIGGRAPH course on 3D scanning</strong></a></p>
<p>Lastly, the <a href="http://en.wikipedia.org/wiki/3D_scanner"><strong>Wikipedia article on 3d scanning</strong></a> is a fairly thorough overview of many different techniques and their uses.</p>

        <hr />
        <center>
            &copy; 2013 David Dohan
            &sdot;
            <a target="_blank" href="mailto:ddohan@princeton.edu">Email</a>
            &sdot;
            <a target="_blank" href="http://www.facebook.com/ddohan">Facebook</a>
            &sdot;
            <a target="_blank" href="http://www.github.com/dmrd">Github</a>
        </center>
    </div>
    </body>
</html>
